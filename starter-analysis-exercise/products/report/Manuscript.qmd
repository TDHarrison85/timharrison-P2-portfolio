---
title: "Data Analysis Project"
subtitle: ""
author: Timothy Harrison
date: today
format:
  html:
    toc: false
    number-sections: true
    highlight-style: github
bibliography: ../dataanalysis-template-references.bib
csl: ../apa.csl
---

```{r, echo=FALSE, message=FALSE}
# load a few R packages
library(here)
library(knitr)
```

# Summary/Abstract

When financial institutions want to extend credit they need to ascertain a customer's credit worthiness and probability of repaying the loan. Companies like Experian exist and can provide information that will aide in this decision. To have a more competitive edge banks can assess the credit worthiness of its customer's with this data, but also have access to their bank information. I want to create a way to segment customers that combines information gleaned from Experian data, like a FICO score, and use data only available to the bank to have a more holisitic way to assess credit worthiness.

{{< pagebreak >}}

# Introduction

## General Background Information

Creditworthiness assessment is a critical process for financial institutions when deciding whether to extend credit to potential borrowers. Traditional methods rely heavily on credit scores provided by agencies like Experian, which offer a numerical representation of a customer's credit history and risk. However, these scores alone may not provide a comprehensive view of a borrower's financial situation. Banks possess additional data about their customers, such as account balances, transaction histories, and other financial behaviors, which can offer deeper insights into their creditworthiness.

In recent years, there has been a growing interest in combining these external credit scores with internal bank data to create more accurate and holistic credit assessments. This approach can help financial institutions better understand the risk profile of their customers and make more informed lending decisions. By leveraging advanced data analysis techniques, such as machine learning and cluster analysis, banks can identify patterns and segments within their customer base that are not apparent through traditional credit scoring methods alone.

The integration of multiple data sources for creditworthiness assessment not only enhances the predictive accuracy but also provides a competitive advantage in the financial industry. This project aims to explore the potential of combining Experian credit data with bank-specific information to develop a more comprehensive method for assessing customer creditworthiness. Through synthetic data creation and advanced statistical analyses, we seek to demonstrate the benefits and feasibility of this integrated approach.

## Description of data and data source

To conduct this analysis, I will generate synthetic data using artificial intelligence software. The dataset will include attributes typically found in Experian credit reports, such as FICO scores and credit history details, along with additional customer attributes accessible through bank records, such as account balances and transaction histories. This approach allows for a more comprehensive dataset that mimics real-world scenarios. The .qmd file responsible for creating this synthetic data can be found under the Synthetic Data tab of the Project Menu.

## Questions/Hypotheses to be addressed

I can create a more holistic picture of a customer's credit worthiness if I factor in data only avaialable through a bank relationship.

<!-- To cite other work (important everywhere, but likely happens first in -->

<!-- introduction), make sure your references are in the bibtex file specified in the -->

<!-- YAML header above (here `dataanalysis_template_references.bib`) and have the -->

<!-- right bibtex key. Then you can include like this: -->

<!-- Examples of reproducible research projects can for instance be found in -->

<!-- [@mckay2020; @mckay2020a] -->

{{< pagebreak >}}

# Methods

The data will be synthetically created with AI. I will use summary statistics to get an idea of the data and principle componenet analysis combined with cluster anlaysis to create groups.

<!-- ## Data aquisition -->

<!-- The data will be created. -->

<!-- ## Data import and cleaning -->

<!-- _Write code that reads in the file and cleans it so it's ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along._ -->

<!-- ## Statistical analysis -->

<!-- _Explain anything related to your statistical analyses._ -->

{{< pagebreak >}}

<!-- # Results -->

<!-- ## Exploratory/Descriptive analysis -->

<!-- _Use a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project._ -->

<!-- @tbl-summarytable shows a summary of the data. -->

<!-- Note the loading of the data providing a **relative** path using the `../../` notation. (Two dots means a folder up). You never want to specify an **absolute** path like `C:\ahandel\myproject\results\` because if you share this with someone, it won't work for them since they don't have that path. You can also use the `here` R package to create paths. See examples of that below. **I recommend the `here` package, but I'm showing the other approach here just in case you encounter it.** -->

<!-- ```{r} -->

<!-- #| label: tbl-summarytable -->

<!-- #| tbl-cap: "Data summary table." -->

<!-- #| echo: FALSE -->

<!-- resulttable=readRDS("../../results/tables-files/summarytable.rds") -->

<!-- knitr::kable(resulttable) -->

<!-- ``` -->

## Basic statistical analysis

```{r}
# Load necessary libraries
library(tidyverse)
library(tidymodels)
library(here)
library(cluster)  # For clustering
library(factoextra)  # For visualizing clusters

# Load the data
data_path <- here("starter-analysis-exercise", "data", "raw-data", "synthetic_financial_data_recreated.csv")
df <- read_csv(data_path)

# Summary statistics
summary_stats <- df %>%
  summarize(
    avg_fico = mean(`FICO Score`, na.rm = TRUE),
    avg_income = mean(`Gross Monthly Income ($)`, na.rm = TRUE),
    avg_obligations = mean(`Current Credit Obligations ($)`, na.rm = TRUE)
)

print(summary_stats)

# Create histograms for FICO Score, Current Credit Obligations, and Gross Monthly Income
fico_hist <- ggplot(df, aes(x = `FICO Score`)) + 
  geom_histogram(binwidth = 10) + 
  ggtitle("FICO Score Distribution")

income_hist <- ggplot(df, aes(x = `Gross Monthly Income ($)`)) + 
  geom_histogram(binwidth = 100) + 
  ggtitle("Gross Monthly Income Distribution")

obligations_hist <- ggplot(df, aes(x = `Current Credit Obligations ($)`)) + 
  geom_histogram(binwidth = 500) + 
  ggtitle("Current Credit Obligations Distribution")

# Display histograms
print(fico_hist)
print(income_hist)
print(obligations_hist)

# Rename columns to simpler names
df_clean <- df %>%
  rename(
    fico_score = `FICO Score`,
    monthly_income = `Gross Monthly Income ($)`,
    credit_obligations = `Current Credit Obligations ($)`,
    debt_write_off = `Debt Write-off`,
    bnpl_3m = `BNPL Usage 3m`,
    bnpl_6m = `BNPL Usage 6m`,
    bnpl_9m = `BNPL Usage 9m`,
    bnpl_12m = `BNPL Usage 12m`
  )

# Ensure the dependent variable is a factor
df_clean <- df_clean %>%
  mutate(debt_write_off = factor(debt_write_off, levels = c(0, 1)))

# Check the structure of df_clean
str(df_clean)

# Analysis 1: Clustering with FICO score, current credit obligations, and payment timeliness

# Select relevant columns for clustering
df_clustering_1 <- df_clean %>%
  select(fico_score, credit_obligations, `Payment Timeliness 30+`, `Payment Timeliness 60+`, `Payment Timeliness 90+`)

# Determine the optimal number of clusters using the elbow method
set.seed(123)
fviz_nbclust(df_clustering_1, kmeans, method = "wss")

# Apply K-means clustering with the chosen number of clusters (3)
set.seed(123)
kmeans_result_1 <- kmeans(df_clustering_1, centers = 3, nstart = 25)

# Add the cluster assignments to the original data
df_clean$cluster_1 <- as.factor(kmeans_result_1$cluster)

# Visualize the clusters
fviz_cluster(kmeans_result_1, data = df_clustering_1, geom = "point") +
  ggtitle("Customer Segments (FICO, Obligations, Payment Timeliness)")

# Print cluster centroids
print(kmeans_result_1$centers)

# Compute summary statistics for each cluster
cluster_summary_1 <- df_clean %>%
  group_by(cluster_1) %>%
  summarize(
    avg_fico = mean(fico_score),
    avg_obligations = mean(credit_obligations),
    avg_payment_30 = mean(`Payment Timeliness 30+`),
    avg_payment_60 = mean(`Payment Timeliness 60+`),
    avg_payment_90 = mean(`Payment Timeliness 90+`),
    count = n()
  )

print(cluster_summary_1)

# Boxplots for FICO Score by cluster
ggplot(df_clean, aes(x = cluster_1, y = fico_score)) +
  geom_boxplot() +
  ggtitle("FICO Score Distribution by Cluster (FICO, Obligations, Payment Timeliness)")

# Boxplots for Current Credit Obligations by cluster
ggplot(df_clean, aes(x = cluster_1, y = credit_obligations)) +
  geom_boxplot() +
  ggtitle("Credit Obligations Distribution by Cluster (FICO, Obligations, Payment Timeliness)")

# Boxplots for Payment Timeliness 30+ by cluster
ggplot(df_clean, aes(x = cluster_1, y = `Payment Timeliness 30+`)) +
  geom_boxplot() +
  ggtitle("Payment Timeliness 30+ Distribution by Cluster (FICO, Obligations, Payment Timeliness)")

# Boxplots for Payment Timeliness 60+ by cluster
ggplot(df_clean, aes(x = cluster_1, y = `Payment Timeliness 60+`)) +
  geom_boxplot() +
  ggtitle("Payment Timeliness 60+ Distribution by Cluster (FICO, Obligations, Payment Timeliness)")

# Boxplots for Payment Timeliness 90+ by cluster
ggplot(df_clean, aes(x = cluster_1, y = `Payment Timeliness 90+`)) +
  geom_boxplot() +
  ggtitle("Payment Timeliness 90+ Distribution by Cluster (FICO, Obligations, Payment Timeliness)")

# Cluster Interpretation
cluster_summary_1 %>%
  mutate(
    interpretation = case_when(
      cluster_1 == 1 ~ "High FICO score, low credit obligations, good payment timeliness",
      cluster_1 == 2 ~ "Moderate FICO score, moderate credit obligations, mixed payment timeliness",
      cluster_1 == 3 ~ "Low FICO score, high credit obligations, poor payment timeliness",
      TRUE ~ "Other"
    )
  )

# Analysis 2: Clustering with BNPL usage variables included

# Select relevant columns for clustering, including BNPL usage
df_clustering_2 <- df_clean %>%
  select(fico_score, credit_obligations, `Payment Timeliness 30+`, `Payment Timeliness 60+`, `Payment Timeliness 90+`, bnpl_3m, bnpl_6m, bnpl_9m, bnpl_12m)

# Determine the optimal number of clusters using the elbow method
set.seed(123)
fviz_nbclust(df_clustering_2, kmeans, method = "wss")

# Apply K-means clustering with the chosen number of clusters (6)
set.seed(123)
kmeans_result_2 <- kmeans(df_clustering_2, centers = 6, nstart = 25)

# Add the cluster assignments to the original data
df_clean$cluster_2 <- as.factor(kmeans_result_2$cluster)

# Visualize the clusters
fviz_cluster(kmeans_result_2, data = df_clustering_2, geom = "point") +
  ggtitle("Customer Segments (FICO, Obligations, Payment Timeliness, BNPL Usage)")

# Print cluster centroids
print(kmeans_result_2$centers)

# Compute summary statistics for each cluster
cluster_summary_2 <- df_clean %>%
  group_by(cluster_2) %>%
  summarize(
    avg_fico = mean(fico_score),
    avg_obligations = mean(credit_obligations),
    avg_payment_30 = mean(`Payment Timeliness 30+`),
    avg_payment_60 = mean(`Payment Timeliness 60+`),
    avg_payment_90 = mean(`Payment Timeliness 90+`),
    avg_bnpl_3m = mean(bnpl_3m),
    avg_bnpl_6m = mean(bnpl_6m),
    avg_bnpl_9m = mean(bnpl_9m),
    avg_bnpl_12m = mean(bnpl_12m),
    count = n()
  )

print(cluster_summary_2)

# Boxplots for BNPL Usage by cluster
ggplot(df_clean, aes(x = cluster_2, y = bnpl_3m)) +
  geom_boxplot() +
  ggtitle("BNPL Usage 3m Distribution by Cluster (FICO, Obligations, Payment Timeliness, BNPL Usage)")

ggplot(df_clean, aes(x = cluster_2, y = bnpl_6m)) +
  geom_boxplot() +
  ggtitle("BNPL Usage 6m Distribution by Cluster (FICO, Obligations, Payment Timeliness, BNPL Usage)")

ggplot(df_clean, aes(x = cluster_2, y = bnpl_9m)) +
  geom_boxplot() +
  ggtitle("BNPL Usage 9m Distribution by Cluster (FICO, Obligations, Payment Timeliness, BNPL Usage)")

ggplot(df_clean, aes(x = cluster_2, y = bnpl_12m)) +
  geom_boxplot() +
  ggtitle("BNPL Usage 12m Distribution by Cluster (FICO, Obligations, Payment Timeliness, BNPL Usage)")

# Cluster Interpretation
cluster_summary_2 %>%
  mutate(
    interpretation = case_when(
      cluster_2 == 1 ~ "Low BNPL usage, good credit profile",
      cluster_2 == 2 ~ "Moderate BNPL usage, moderate credit profile",
      cluster_2 == 3 ~ "High BNPL usage, poor credit profile",
      cluster_2 == 4 ~ "High BNPL usage, good credit profile",
      cluster_2 == 5 ~ "Moderate BNPL usage, mixed credit profile",
      cluster_2 == 6 ~ "Low BNPL usage, mixed credit profile",
      TRUE ~ "Other"
    )
  )

# Logistic regression model using clusters as a predictor
logistic_model <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(debt_write_off ~ cluster_1 + cluster_2, data = df_clean)

# Display logistic model summary
logistic_summary <- summary(logistic_model$fit)
print(logistic_summary)

```

```{}
```

```{}
```

<!-- _To get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any "p<0.05 means statistical significance" interpretation is not valid._ -->

<!-- @fig-result shows a scatterplot figure produced by one of the R scripts. -->

<!-- ```{r} -->

<!-- #| label: fig-result -->

<!-- #| fig-cap: "Height and weight stratified by gender." -->

<!-- #| echo: FALSE -->

<!-- knitr::include_graphics(here("starter-analysis-exercise","results","figures","height-weight-stratified.png")) -->

<!-- ``` -->

<!-- ## Full analysis -->

<!-- _Use one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here._ -->

<!-- Example @tbl-resulttable2 shows a summary of a linear model fit. -->

<!-- ```{r} -->

<!-- #| label: tbl-resulttable2 -->

<!-- #| tbl-cap: "Linear model fit table." -->

<!-- #| echo: FALSE -->

<!-- resulttable2 = readRDS(here("starter-analysis-exercise","results","tables-files","resulttable2.rds")) -->

<!-- knitr::kable(resulttable2) -->

<!-- ``` -->

<!-- {{< pagebreak >}} -->

<!-- # Discussion -->

<!-- ## Summary and Interpretation -->

<!-- _Summarize what you did, what you found and what it means._ -->

<!-- ## Strengths and Limitations -->

<!-- _Discuss what you perceive as strengths and limitations of your analysis._ -->

<!-- ## Conclusions -->

<!-- _What are the main take-home messages?_ -->

<!-- _Include citations in your Rmd file using bibtex, the list of references will automatically be placed at the end_ -->

<!-- This paper [@leek2015] discusses types of analyses.  -->

<!-- These papers [@mckay2020; @mckay2020a] are good examples of papers published using a fully reproducible setup similar to the one shown in this template.  -->

<!-- Note that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal [are available](https://www.zotero.org/styles). You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word `references.bib` but giving it a more descriptive name is probably better. -->

<!-- {{< pagebreak >}} -->

<!-- # References -->
